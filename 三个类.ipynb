{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "三个类.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/wtyn/AI-Practice-Tensorflow-Notes/blob/master/%E4%B8%89%E4%B8%AA%E7%B1%BB.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1r9YHJDrMYew",
        "colab_type": "code",
        "outputId": "0afb88b1-6337-4fdc-ac18-67e4ca3831e9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "# 运行此单元格即可装载您的 Google 云端硬盘。\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yK3J63gHMV-D",
        "colab_type": "text"
      },
      "source": [
        "# 数据"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w1ygh3CFM4yp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.model_selection import train_test_split\n",
        "import random\n",
        "\n",
        "\n",
        "image_size = 197\n",
        "class_num = 3\n",
        "is_gpu = True"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LrS50_yrLB5O",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "def parse_exmp(serial_exmp):\n",
        "    # filename_queue = tf.train.string_input_producer(file_list)  # 建立一个队列，其中的参数为tfrecords文件的路径\n",
        "    # reader = tf.TFRecordReader()  # 实例化读操作，建立读取器\n",
        "    # _, serialized_example = reader.read(filename_queue)  # 返回文件名和文件\n",
        "\n",
        "    keys_to_features = {\n",
        "        'image': tf.FixedLenFeature([], tf.string),\n",
        "        'label': tf.FixedLenFeature([], tf.int64)\n",
        "    }\n",
        "\n",
        "    feats = tf.parse_single_example(serial_exmp, keys_to_features)\n",
        "\n",
        "    # image = tf.decode_raw(feats['image'], tf.uint8)\n",
        "    # label = feats['label']\n",
        "\n",
        "    image = tf.decode_raw(feats['image'], tf.uint8)\n",
        "    \n",
        "    label = tf.cast(feats['label'], tf.int64)\n",
        "    \n",
        "    return image, label\n",
        "\n",
        "\n",
        "def _preprocess_image(image, label):\n",
        "    \n",
        "    _image = preprocess_input(image)\n",
        "    \n",
        "    return _image, label\n",
        "    \n",
        "\n",
        "\n",
        "def get_dataset(file_list, shuffle_num=0, batch_size=32):\n",
        "    \n",
        "    \n",
        "    dataset = tf.data.TFRecordDataset(file_list)\n",
        "\n",
        "    # Maps the parser on every filepath in the array. You can set the number of parallel loaders here\n",
        "    dataset = dataset.map(parse_exmp)\n",
        "    \n",
        "    # 使用lambda函数处理(归一化)\n",
        "#     dataset = dataset.map(lambda image, label : (preprocess_input(image), label))\n",
        "#     dataset = dataset.map(lambda image, label: preprocess_image(image, label))\n",
        "    \n",
        "#     dataset = dataset.map(_preprocess_image)\n",
        "    \n",
        "    \n",
        "    \n",
        "    # This dataset will go on forever\n",
        "\n",
        "    dataset = dataset.repeat()\n",
        "\n",
        "    # Set the number of datapoints you want to load and shuffle\n",
        "    if shuffle_num > 0:\n",
        "        dataset = dataset.shuffle(shuffle_num)\n",
        "\n",
        "    # Set the batchsize\n",
        "\n",
        "    dataset = dataset.batch(batch_size)\n",
        "\n",
        "    # Create an iterator\n",
        "\n",
        "    iterator = dataset.make_one_shot_iterator()\n",
        "\n",
        "    # Create your tf representation of the iterator\n",
        "    \n",
        "    image, label = iterator.get_next()\n",
        "    \n",
        "    \n",
        "    # Bring your picture back in shape\n",
        "    \n",
        "    image = tf.reshape(image, [-1, image_size, image_size, class_num])\n",
        "    image = tf.cast(image, tf.float32)\n",
        "    \n",
        "    image =tf.keras.applications.resnet50. preprocess_input(image)\n",
        "    \n",
        "    # Create a one hot array for your labels\n",
        "    \n",
        "    label = tf.one_hot(label, class_num)\n",
        "    \n",
        "    return image, label\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JyF0s1YFM-y4",
        "colab_type": "code",
        "outputId": "8f9491a2-4a6d-4c5b-a75e-fa51beac20f9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "\n",
        "\n",
        "# 数据集\n",
        "data_file = '/content/drive/My Drive/dataset/三分类/cats_dogs_panda_197_197.tfrecords'\n",
        "\n",
        "x, y= get_dataset([data_file], batch_size=1, shuffle_num=0)\n",
        "\n",
        "\n",
        "x_ = []\n",
        "y_ = []\n",
        "\n",
        "with tf.Session() as sess:\n",
        "    init_op = tf.global_variables_initializer()\n",
        "    sess.run(init_op)\n",
        "    for i in range(3000):\n",
        "        image, label = sess.run((x, y))\n",
        "        \n",
        "        \n",
        "        x_.append(image)\n",
        "        y_.append(label)\n",
        "\n",
        "        \n",
        "x_ = np.array(x_).reshape((-1, image_size, image_size, 3))\n",
        "\n",
        "\n",
        "y_ = np.array(y_).reshape((-1, class_num))\n",
        "\n",
        "print(x_.shape)\n",
        "print(y_.shape)\n",
        "\n",
        "\n",
        "# 打乱数据集\n",
        "random.seed(88)\n",
        "index = [i for i in range(len(x_))]\n",
        "random.shuffle(index)\n",
        "\n",
        "x_ = x_[index]\n",
        "y_ = y_[index]\n",
        "\n",
        "\n",
        "(x_train, x_test, y_train, y_test) = train_test_split(x_,\n",
        "                                                  y_, test_size=0.25, random_state=88)\n",
        "\n",
        "print('------------')\n",
        "print(x_train.shape)\n",
        "print(y_train.shape)\n",
        "print(x_test.shape)\n",
        "print(y_test.shape)\n",
        "print(x_train[0])\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING: Logging before flag parsing goes to stderr.\n",
            "W0629 05:37:48.239406 139712764553088 deprecation.py:323] From <ipython-input-4-18c37120dd4e>:62: DatasetV1.make_one_shot_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_one_shot_iterator(dataset)`.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "(3000, 197, 197, 3)\n",
            "(3000, 3)\n",
            "------------\n",
            "(2250, 197, 197, 3)\n",
            "(2250, 3)\n",
            "(750, 197, 197, 3)\n",
            "(750, 3)\n",
            "[[[ -58.939003   -47.779      -12.68     ]\n",
            "  [ -57.939003   -39.779      -15.68     ]\n",
            "  [ -38.939003   -12.778999    -5.6800003]\n",
            "  ...\n",
            "  [ -94.939     -103.779      -91.68     ]\n",
            "  [ -59.939003   -42.779      -30.68     ]\n",
            "  [ -61.939003   -66.779      -56.68     ]]\n",
            "\n",
            " [[ -26.939003   -11.778999     5.3199997]\n",
            "  [ -36.939003   -22.779       -2.6800003]\n",
            "  [ -56.939003   -43.779      -14.68     ]\n",
            "  ...\n",
            "  [ -84.939      -68.779      -60.68     ]\n",
            "  [ -90.939      -84.779      -78.68     ]\n",
            "  [ -62.939003   -46.779      -48.68     ]]\n",
            "\n",
            " [[ -33.939003   -13.778999     5.3199997]\n",
            "  [ -32.939003   -11.778999     4.3199997]\n",
            "  [ -38.939003   -25.779        3.3199997]\n",
            "  ...\n",
            "  [ -78.939      -89.779      -72.68     ]\n",
            "  [ -67.939      -61.779      -53.68     ]\n",
            "  [-101.939      -81.779      -85.68     ]]\n",
            "\n",
            " ...\n",
            "\n",
            " [[ -64.939      -11.778999   -67.68     ]\n",
            "  [ -62.939003    -0.7789993  -59.68     ]\n",
            "  [ -36.939003    17.221      -44.68     ]\n",
            "  ...\n",
            "  [ -95.939      -97.779      -90.68     ]\n",
            "  [ -99.939     -103.779     -107.68     ]\n",
            "  [ -76.939      -89.779      -82.68     ]]\n",
            "\n",
            " [[ -86.939      -24.779      -69.68     ]\n",
            "  [-103.939      -61.779     -107.68     ]\n",
            "  [-101.939      -64.779     -101.68     ]\n",
            "  ...\n",
            "  [ -57.939003   -55.779      -30.68     ]\n",
            "  [ -74.939      -73.779      -57.68     ]\n",
            "  [ -64.939      -48.779      -40.68     ]]\n",
            "\n",
            " [[-103.939      -59.779      -99.68     ]\n",
            "  [ -96.939      -55.779     -109.68     ]\n",
            "  [-103.939      -92.779     -109.68     ]\n",
            "  ...\n",
            "  [ -82.939      -59.779      -42.68     ]\n",
            "  [ -79.939      -73.779      -72.68     ]\n",
            "  [ -79.939      -78.779      -73.68     ]]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cbmktLA9LpJZ",
        "colab_type": "text"
      },
      "source": [
        "# 模型"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jpEoEX4QNzVO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Trainsport_resnet50:\n",
        "    \n",
        "    @staticmethod\n",
        "    def build():\n",
        "        \n",
        "        # base_model = ResNet50(weights='imagenet', include_top=False, pooling=None,\n",
        "        #                       input_shape=(img_weight, img_height, color), classes=nb_classes)\n",
        "        base_model_weights_path = '/content/drive/My Drive/model/resnet50_weights_tf_dim_ordering_tf_kernels_notop.h5'\n",
        "        base_model = tf.keras.applications.ResNet50(weights='imagenet', include_top=False, \n",
        "                              input_shape=(image_size, image_size, 3), \n",
        "                              pooling='avg',\n",
        "                              classes=class_num)\n",
        "\n",
        "        for layer in base_model.layers:\n",
        "            layer.trainable = False\n",
        "\n",
        "        x = base_model.output\n",
        "\n",
        "#         x = Flatten()(x)\n",
        "\n",
        "#         x = GlobalAveragePooling2D()(x)\n",
        "#         x = tf.keras.layers.Dropout(0.5)(x)\n",
        "        \n",
        "#         x = tf.keras.layers.Dense(320, activation='relu',\n",
        "#                  kernel_initializer=tf.keras.initializers.TruncatedNormal(mean=0.0, stddev=0.01))(x)\n",
        "        \n",
        "#         x = tf.keras.layers.Dropout(0.5)(x)\n",
        "        \n",
        "        \n",
        "        predictions = tf.keras.layers.Dense(class_num, activation='softmax', \n",
        "                            kernel_initializer=tf.keras.initializers.TruncatedNormal(mean=0.0, stddev=0.01))(x)\n",
        "\n",
        "        inputs = base_model.input\n",
        "        outputs = predictions\n",
        "\n",
        "        model = tf.keras.Model(inputs=inputs, outputs=outputs)\n",
        "        \n",
        "#         model.summary()\n",
        "        \n",
        "        return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_ZVeXGzoLrPB",
        "colab_type": "text"
      },
      "source": [
        "# 训练"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RHNGEE4zP_iC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "val_loss = 1000.\n",
        "\n",
        "\n",
        "# 定义回调\n",
        "class LossHistory(tf.keras.callbacks.Callback):\n",
        "    \n",
        "        \n",
        "    def on_train_begin(self, logs):\n",
        "        self.losses = []\n",
        "        self.acces = []\n",
        "        self.val_losses = []\n",
        "        self.val_acces = []\n",
        "    \n",
        "        \n",
        "    def on_epoch_end(self, batch, logs):\n",
        "        print('-----')\n",
        "        print(logs)\n",
        "        self.losses.append(logs.get('loss'))\n",
        "        self.acces.append(logs.get('acc'))\n",
        "        self.val_losses.append(logs.get('val_loss'))\n",
        "        self.val_acces.append(logs.get('val_acc'))\n",
        "        \n",
        "#         print('\\nloss:',self.losses[-1])\n",
        "        global val_loss\n",
        "        if logs.get('val_loss') < val_loss:\n",
        "            val_loss = logs.get('val_loss')\n",
        "            self.model.save('cats_dogs_min_loss_model.h5')\n",
        "            print(val_loss)\n",
        "        \n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L6rIug0oPp7x",
        "colab_type": "code",
        "outputId": "5b86d228-c8a4-4579-eb8d-61781581a061",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "\n",
        "H = None\n",
        "\n",
        "BATCH_SIZE = 64\n",
        "EPOCHES = 500\n",
        "\n",
        "\n",
        "if is_gpu:\n",
        "    \n",
        "    model = Trainsport_resnet50.build()\n",
        "    \n",
        "    adam = tf.keras.optimizers.Adam(1e-4)\n",
        "    \n",
        "    model.compile(loss=\"categorical_crossentropy\",  \n",
        "              optimizer=adam,\n",
        "\t          metrics=[\"accuracy\"])\n",
        "\n",
        "\n",
        "#     es = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=10)\n",
        "    mc = tf.keras.callbacks.ModelCheckpoint('cats_dogs_best_model.h5', monitor='val_loss', save_best_only=True)\n",
        "    \n",
        "\n",
        "    \n",
        "#     lossHistory = LossHistory()\n",
        "#     lossHistory.set_model(model)\n",
        "    \n",
        "    aug = tf.keras.preprocessing.image.ImageDataGenerator(rotation_range=90, width_shift_range=0.1,\n",
        "                         height_shift_range=0.1, shear_range=0.2, zoom_range=0.2,\n",
        "                         horizontal_flip=True, fill_mode=\"nearest\"\n",
        "                           )\n",
        "    \n",
        "    \n",
        "    \n",
        "    H = model.fit(aug.flow(x_train, y_train), \n",
        "            epochs=EPOCHES, \n",
        "            validation_data=(x_test, y_test), \n",
        "            callbacks=[mc]\n",
        "         )\n",
        "    \n",
        "#     H = model.fit(x_train, y_train, \n",
        "#             epochs=EPOCHES, \n",
        "#             validation_data=(x_test, y_test), \n",
        "#             callbacks=[mc]\n",
        "#          )\n",
        "    \n",
        "    \n",
        "    \n",
        "   \n",
        "\n",
        "#     H = model.fit(x_train, y_train,\n",
        "#                    epochs=100, \n",
        "#                    steps_per_epoch=100, \n",
        "#                    validation_data=(x_test, y_test),\n",
        "#                    validation_steps=4004//32,\n",
        "#                    validation_freq=5\n",
        "#                   )\n",
        " \n",
        "\n",
        "# model.fit(train_x, train_y, epochs=20, steps_per_epoch=100, validation_data=(test_x, test_y), validation_freq=10)\n",
        "\n",
        "\n",
        "    \n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://github.com/fchollet/deep-learning-models/releases/download/v0.2/resnet50_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "94658560/94653016 [==============================] - 1s 0us/step\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "W0629 05:41:49.405825 139712764553088 deprecation.py:506] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/initializers.py:94: calling TruncatedNormal.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Call initializer instance with the dtype argument instead of passing it to the constructor\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/500\n",
            "71/71 [==============================] - 31s 440ms/step - loss: 0.8246 - acc: 0.7400 - val_loss: 0.3748 - val_acc: 0.9467\n",
            "Epoch 2/500\n",
            "71/71 [==============================] - 18s 256ms/step - loss: 0.5095 - acc: 0.8996 - val_loss: 0.1950 - val_acc: 0.9773\n",
            "Epoch 3/500\n",
            "71/71 [==============================] - 19s 265ms/step - loss: 0.3869 - acc: 0.9147 - val_loss: 0.1422 - val_acc: 0.9747\n",
            "Epoch 4/500\n",
            "71/71 [==============================] - 19s 268ms/step - loss: 0.3317 - acc: 0.9169 - val_loss: 0.1024 - val_acc: 0.9800\n",
            "Epoch 5/500\n",
            "71/71 [==============================] - 19s 263ms/step - loss: 0.2788 - acc: 0.9356 - val_loss: 0.0892 - val_acc: 0.9813\n",
            "Epoch 6/500\n",
            "71/71 [==============================] - 19s 265ms/step - loss: 0.2546 - acc: 0.9324 - val_loss: 0.0827 - val_acc: 0.9760\n",
            "Epoch 7/500\n",
            "71/71 [==============================] - 19s 263ms/step - loss: 0.2426 - acc: 0.9302 - val_loss: 0.0719 - val_acc: 0.9827\n",
            "Epoch 8/500\n",
            "71/71 [==============================] - 19s 262ms/step - loss: 0.2177 - acc: 0.9413 - val_loss: 0.0624 - val_acc: 0.9907\n",
            "Epoch 9/500\n",
            "71/71 [==============================] - 18s 259ms/step - loss: 0.2016 - acc: 0.9422 - val_loss: 0.0671 - val_acc: 0.9760\n",
            "Epoch 10/500\n",
            "71/71 [==============================] - 19s 265ms/step - loss: 0.1936 - acc: 0.9462 - val_loss: 0.0578 - val_acc: 0.9853\n",
            "Epoch 11/500\n",
            "71/71 [==============================] - 19s 265ms/step - loss: 0.1916 - acc: 0.9396 - val_loss: 0.0523 - val_acc: 0.9880\n",
            "Epoch 12/500\n",
            "71/71 [==============================] - 19s 263ms/step - loss: 0.1787 - acc: 0.9471 - val_loss: 0.0514 - val_acc: 0.9867\n",
            "Epoch 13/500\n",
            "71/71 [==============================] - 18s 257ms/step - loss: 0.1681 - acc: 0.9498 - val_loss: 0.0570 - val_acc: 0.9813\n",
            "Epoch 14/500\n",
            "71/71 [==============================] - 19s 267ms/step - loss: 0.1690 - acc: 0.9476 - val_loss: 0.0504 - val_acc: 0.9840\n",
            "Epoch 15/500\n",
            "71/71 [==============================] - 19s 265ms/step - loss: 0.1687 - acc: 0.9453 - val_loss: 0.0500 - val_acc: 0.9827\n",
            "Epoch 16/500\n",
            "71/71 [==============================] - 19s 262ms/step - loss: 0.1577 - acc: 0.9498 - val_loss: 0.0466 - val_acc: 0.9867\n",
            "Epoch 17/500\n",
            "71/71 [==============================] - 18s 256ms/step - loss: 0.1585 - acc: 0.9498 - val_loss: 0.0517 - val_acc: 0.9813\n",
            "Epoch 18/500\n",
            "71/71 [==============================] - 19s 266ms/step - loss: 0.1624 - acc: 0.9498 - val_loss: 0.0407 - val_acc: 0.9867\n",
            "Epoch 19/500\n",
            "71/71 [==============================] - 18s 254ms/step - loss: 0.1572 - acc: 0.9462 - val_loss: 0.0459 - val_acc: 0.9840\n",
            "Epoch 20/500\n",
            "71/71 [==============================] - 18s 258ms/step - loss: 0.1556 - acc: 0.9453 - val_loss: 0.0501 - val_acc: 0.9827\n",
            "Epoch 21/500\n",
            "71/71 [==============================] - 18s 259ms/step - loss: 0.1492 - acc: 0.9484 - val_loss: 0.0444 - val_acc: 0.9840\n",
            "Epoch 22/500\n",
            "71/71 [==============================] - 19s 266ms/step - loss: 0.1427 - acc: 0.9547 - val_loss: 0.0398 - val_acc: 0.9880\n",
            "Epoch 23/500\n",
            "71/71 [==============================] - 18s 257ms/step - loss: 0.1346 - acc: 0.9591 - val_loss: 0.0404 - val_acc: 0.9827\n",
            "Epoch 24/500\n",
            "71/71 [==============================] - 18s 259ms/step - loss: 0.1304 - acc: 0.9622 - val_loss: 0.0453 - val_acc: 0.9827\n",
            "Epoch 25/500\n",
            "71/71 [==============================] - 18s 258ms/step - loss: 0.1427 - acc: 0.9480 - val_loss: 0.0499 - val_acc: 0.9800\n",
            "Epoch 26/500\n",
            "71/71 [==============================] - 18s 260ms/step - loss: 0.1292 - acc: 0.9564 - val_loss: 0.0437 - val_acc: 0.9827\n",
            "Epoch 27/500\n",
            "71/71 [==============================] - 18s 259ms/step - loss: 0.1300 - acc: 0.9573 - val_loss: 0.0410 - val_acc: 0.9867\n",
            "Epoch 28/500\n",
            "71/71 [==============================] - 18s 259ms/step - loss: 0.1243 - acc: 0.9609 - val_loss: 0.0415 - val_acc: 0.9827\n",
            "Epoch 29/500\n",
            "71/71 [==============================] - 18s 258ms/step - loss: 0.1255 - acc: 0.9578 - val_loss: 0.0426 - val_acc: 0.9840\n",
            "Epoch 30/500\n",
            "71/71 [==============================] - 18s 259ms/step - loss: 0.1327 - acc: 0.9520 - val_loss: 0.0488 - val_acc: 0.9813\n",
            "Epoch 31/500\n",
            "71/71 [==============================] - 19s 265ms/step - loss: 0.1195 - acc: 0.9662 - val_loss: 0.0396 - val_acc: 0.9867\n",
            "Epoch 32/500\n",
            "71/71 [==============================] - 18s 255ms/step - loss: 0.1163 - acc: 0.9680 - val_loss: 0.0407 - val_acc: 0.9840\n",
            "Epoch 33/500\n",
            "71/71 [==============================] - 19s 265ms/step - loss: 0.1189 - acc: 0.9582 - val_loss: 0.0349 - val_acc: 0.9893\n",
            "Epoch 34/500\n",
            "71/71 [==============================] - 18s 257ms/step - loss: 0.1276 - acc: 0.9542 - val_loss: 0.0421 - val_acc: 0.9867\n",
            "Epoch 35/500\n",
            "71/71 [==============================] - 18s 258ms/step - loss: 0.1249 - acc: 0.9547 - val_loss: 0.0473 - val_acc: 0.9813\n",
            "Epoch 36/500\n",
            "71/71 [==============================] - 18s 256ms/step - loss: 0.1220 - acc: 0.9587 - val_loss: 0.0395 - val_acc: 0.9867\n",
            "Epoch 37/500\n",
            "71/71 [==============================] - 19s 268ms/step - loss: 0.1173 - acc: 0.9587 - val_loss: 0.0359 - val_acc: 0.9880\n",
            "Epoch 38/500\n",
            "71/71 [==============================] - 18s 257ms/step - loss: 0.1098 - acc: 0.9653 - val_loss: 0.0376 - val_acc: 0.9880\n",
            "Epoch 39/500\n",
            "71/71 [==============================] - 19s 261ms/step - loss: 0.1126 - acc: 0.9604 - val_loss: 0.0418 - val_acc: 0.9827\n",
            "Epoch 40/500\n",
            "71/71 [==============================] - 19s 265ms/step - loss: 0.1115 - acc: 0.9640 - val_loss: 0.0346 - val_acc: 0.9893\n",
            "Epoch 41/500\n",
            "71/71 [==============================] - 18s 255ms/step - loss: 0.1172 - acc: 0.9573 - val_loss: 0.0404 - val_acc: 0.9840\n",
            "Epoch 42/500\n",
            "71/71 [==============================] - 19s 264ms/step - loss: 0.1204 - acc: 0.9618 - val_loss: 0.0291 - val_acc: 0.9907\n",
            "Epoch 43/500\n",
            "71/71 [==============================] - 18s 255ms/step - loss: 0.1035 - acc: 0.9627 - val_loss: 0.0403 - val_acc: 0.9853\n",
            "Epoch 44/500\n",
            "71/71 [==============================] - 18s 259ms/step - loss: 0.1164 - acc: 0.9582 - val_loss: 0.0329 - val_acc: 0.9893\n",
            "Epoch 45/500\n",
            "71/71 [==============================] - 18s 257ms/step - loss: 0.1148 - acc: 0.9618 - val_loss: 0.0356 - val_acc: 0.9880\n",
            "Epoch 46/500\n",
            "71/71 [==============================] - 18s 258ms/step - loss: 0.1159 - acc: 0.9556 - val_loss: 0.0457 - val_acc: 0.9827\n",
            "Epoch 47/500\n",
            "71/71 [==============================] - 18s 258ms/step - loss: 0.1142 - acc: 0.9596 - val_loss: 0.0413 - val_acc: 0.9853\n",
            "Epoch 48/500\n",
            "71/71 [==============================] - 18s 260ms/step - loss: 0.1181 - acc: 0.9564 - val_loss: 0.0371 - val_acc: 0.9880\n",
            "Epoch 49/500\n",
            "71/71 [==============================] - 18s 257ms/step - loss: 0.1086 - acc: 0.9596 - val_loss: 0.0422 - val_acc: 0.9867\n",
            "Epoch 50/500\n",
            "71/71 [==============================] - 18s 258ms/step - loss: 0.1068 - acc: 0.9627 - val_loss: 0.0344 - val_acc: 0.9880\n",
            "Epoch 51/500\n",
            "71/71 [==============================] - 19s 265ms/step - loss: 0.1260 - acc: 0.9529 - val_loss: 0.0364 - val_acc: 0.9880\n",
            "Epoch 52/500\n",
            "71/71 [==============================] - 18s 257ms/step - loss: 0.1074 - acc: 0.9618 - val_loss: 0.0405 - val_acc: 0.9880\n",
            "Epoch 53/500\n",
            "71/71 [==============================] - 18s 258ms/step - loss: 0.1024 - acc: 0.9631 - val_loss: 0.0365 - val_acc: 0.9880\n",
            "Epoch 54/500\n",
            "71/71 [==============================] - 18s 258ms/step - loss: 0.1050 - acc: 0.9618 - val_loss: 0.0366 - val_acc: 0.9880\n",
            "Epoch 55/500\n",
            "71/71 [==============================] - 18s 259ms/step - loss: 0.1049 - acc: 0.9591 - val_loss: 0.0415 - val_acc: 0.9880\n",
            "Epoch 56/500\n",
            "71/71 [==============================] - 18s 258ms/step - loss: 0.0963 - acc: 0.9649 - val_loss: 0.0384 - val_acc: 0.9880\n",
            "Epoch 57/500\n",
            "71/71 [==============================] - 18s 260ms/step - loss: 0.0967 - acc: 0.9658 - val_loss: 0.0396 - val_acc: 0.9880\n",
            "Epoch 58/500\n",
            "71/71 [==============================] - 18s 259ms/step - loss: 0.0933 - acc: 0.9689 - val_loss: 0.0397 - val_acc: 0.9880\n",
            "Epoch 59/500\n",
            "71/71 [==============================] - 18s 260ms/step - loss: 0.0999 - acc: 0.9689 - val_loss: 0.0339 - val_acc: 0.9907\n",
            "Epoch 60/500\n",
            "71/71 [==============================] - 18s 260ms/step - loss: 0.1116 - acc: 0.9622 - val_loss: 0.0372 - val_acc: 0.9880\n",
            "Epoch 61/500\n",
            "71/71 [==============================] - 18s 260ms/step - loss: 0.0994 - acc: 0.9671 - val_loss: 0.0386 - val_acc: 0.9880\n",
            "Epoch 62/500\n",
            "71/71 [==============================] - 18s 259ms/step - loss: 0.0908 - acc: 0.9716 - val_loss: 0.0379 - val_acc: 0.9880\n",
            "Epoch 63/500\n",
            "71/71 [==============================] - 18s 259ms/step - loss: 0.0963 - acc: 0.9653 - val_loss: 0.0322 - val_acc: 0.9907\n",
            "Epoch 64/500\n",
            "71/71 [==============================] - 18s 259ms/step - loss: 0.0938 - acc: 0.9716 - val_loss: 0.0329 - val_acc: 0.9907\n",
            "Epoch 65/500\n",
            "71/71 [==============================] - 18s 260ms/step - loss: 0.0898 - acc: 0.9693 - val_loss: 0.0338 - val_acc: 0.9893\n",
            "Epoch 66/500\n",
            "71/71 [==============================] - 18s 258ms/step - loss: 0.0973 - acc: 0.9622 - val_loss: 0.0357 - val_acc: 0.9893\n",
            "Epoch 67/500\n",
            "71/71 [==============================] - 18s 259ms/step - loss: 0.1050 - acc: 0.9600 - val_loss: 0.0422 - val_acc: 0.9853\n",
            "Epoch 68/500\n",
            "71/71 [==============================] - 18s 259ms/step - loss: 0.0868 - acc: 0.9698 - val_loss: 0.0355 - val_acc: 0.9907\n",
            "Epoch 69/500\n",
            "71/71 [==============================] - 18s 258ms/step - loss: 0.0937 - acc: 0.9649 - val_loss: 0.0336 - val_acc: 0.9893\n",
            "Epoch 70/500\n",
            "71/71 [==============================] - 19s 261ms/step - loss: 0.0970 - acc: 0.9658 - val_loss: 0.0387 - val_acc: 0.9880\n",
            "Epoch 71/500\n",
            "71/71 [==============================] - 18s 260ms/step - loss: 0.0986 - acc: 0.9671 - val_loss: 0.0370 - val_acc: 0.9893\n",
            "Epoch 72/500\n",
            "71/71 [==============================] - 18s 258ms/step - loss: 0.0838 - acc: 0.9724 - val_loss: 0.0441 - val_acc: 0.9840\n",
            "Epoch 73/500\n",
            "71/71 [==============================] - 18s 257ms/step - loss: 0.0921 - acc: 0.9689 - val_loss: 0.0315 - val_acc: 0.9907\n",
            "Epoch 74/500\n",
            "71/71 [==============================] - 18s 259ms/step - loss: 0.0835 - acc: 0.9733 - val_loss: 0.0348 - val_acc: 0.9893\n",
            "Epoch 75/500\n",
            "71/71 [==============================] - 18s 259ms/step - loss: 0.0947 - acc: 0.9622 - val_loss: 0.0416 - val_acc: 0.9867\n",
            "Epoch 76/500\n",
            "71/71 [==============================] - 18s 258ms/step - loss: 0.0808 - acc: 0.9724 - val_loss: 0.0348 - val_acc: 0.9907\n",
            "Epoch 77/500\n",
            "71/71 [==============================] - 19s 267ms/step - loss: 0.0981 - acc: 0.9631 - val_loss: 0.0378 - val_acc: 0.9893\n",
            "Epoch 78/500\n",
            "71/71 [==============================] - 18s 255ms/step - loss: 0.0874 - acc: 0.9702 - val_loss: 0.0358 - val_acc: 0.9893\n",
            "Epoch 79/500\n",
            "71/71 [==============================] - 18s 258ms/step - loss: 0.0719 - acc: 0.9787 - val_loss: 0.0352 - val_acc: 0.9893\n",
            "Epoch 80/500\n",
            "45/71 [==================>...........] - ETA: 5s - loss: 0.0961 - acc: 0.9688"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dhzwyzZQLdPP",
        "colab_type": "text"
      },
      "source": [
        "# TPU训练"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QTpsdqLRflHS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "\n",
        "\n",
        "\n",
        "# tpu训练\n",
        "if not is_gpu:\n",
        "    \n",
        "    \n",
        "    \n",
        "    TPU_WORKER = 'grpc://' + os.environ['COLAB_TPU_ADDR']\n",
        "\n",
        "    resolver = tf.contrib.cluster_resolver.TPUClusterResolver(tpu=TPU_WORKER)\n",
        "\n",
        "    tf.contrib.distribute.initialize_tpu_system(resolver)\n",
        "\n",
        "    strategy = tf.contrib.distribute.TPUStrategy(resolver)\n",
        "\n",
        "\n",
        "    with strategy.scope():\n",
        "        model = Trainsport_resnet50.build()\n",
        "\n",
        "        model.compile(loss=\"categorical_crossentropy\", \n",
        "              optimizer=tf.keras.optimizers.Adam(),\n",
        "\t          metrics=[\"accuracy\"])\n",
        "\n",
        "\n",
        "# model.fit(dataset_train, \n",
        "#           epochs=20, \n",
        "#           steps_per_epoch=100, \n",
        "#           validation_data=dataset_test, \n",
        "#           validation_steps=20, \n",
        "#           validation_freq=5\n",
        "#          )\n",
        "    \n",
        "\n",
        "#     H = model.fit(x_train, y_train, \n",
        "#           epochs=50, \n",
        "#           steps_per_epoch=105, \n",
        "#           validation_data=(x_test, y_test), \n",
        "#           validation_steps=4004//32, \n",
        "#           validation_freq=5\n",
        "#          )\n",
        "    \n",
        "    count = len(x_train)//32*32\n",
        "    x_train = x_train[:count]\n",
        "    y_train = y_train[:count]\n",
        "    H = model.fit(x_train, y_train, \n",
        "          epochs=50, \n",
        "          steps_per_epoch=70\n",
        "          \n",
        "         )\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4NnaVTPZMPM6",
        "colab_type": "text"
      },
      "source": [
        "# 评估"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eLtMHRVcVB5Z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 保存模型到本地\n",
        "print(\"[INFO] 正在保存模型\")\n",
        "model.save('cats_dogs_classify_model.h5')\n",
        "\n",
        "# f = open('label.bin', \"wb\")\n",
        "# f.write(pickle.dumps(labl.bin))\n",
        "# f.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dUwD3pL0LqhJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# 画图\n",
        "def plot_training(history):\n",
        "        acc = history.history['acc']\n",
        "        val_acc = history.history['val_acc']\n",
        "        loss = history.history['loss']\n",
        "        val_loss = history.history['val_loss']\n",
        "        epochs = range(len(acc))\n",
        "        plt.plot(epochs, acc, 'b-')\n",
        "        plt.plot(epochs, val_acc, 'r')\n",
        "        plt.title('Training and validation accuracy')\n",
        "        plt.figure()\n",
        "        \n",
        "        plt.plot(epochs, loss, 'b-')\n",
        "        plt.plot(epochs, val_loss, 'r-')\n",
        "        plt.title('Training and validation loss')\n",
        "        plt.show()\n",
        "        \n",
        "\n",
        "plot_training(H)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dTBlYAxMVM1v",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "\n",
        "# 测试网络模型\n",
        "def evaluation(model_, x_test, y_test):\n",
        "    \n",
        "#     x_test = preprocess_input(x_test)\n",
        "    \n",
        "    \n",
        "    print(\"[INFO] 正在评估模型\")\n",
        "\n",
        "    predictions = model_.predict(x_test, batch_size=200)\n",
        "\n",
        "    \n",
        "    evaluation_score = classification_report(y_test.argmax(axis=1),\n",
        "                                             predictions.argmax(axis=1), target_names=['cats', 'dogs', 'panda'])\n",
        "\n",
        "    print(evaluation_score)\n",
        "\n",
        "evaluation(model, x_test, y_test)\n",
        "\n",
        "print('----->>>>>>')\n",
        "\n",
        "# 最好模型预测\n",
        "model_best = Trainsport_resnet50.build()\n",
        "model_best.load_weights('cats_dogs_best_model.h5')\n",
        "evaluation(model_best, x_test, y_test)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H_x1GOiL2lR9",
        "colab_type": "text"
      },
      "source": [
        "# 直接加载模型进行评估"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "397QyG8J18Bm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model_ = tf.keras.models.load_model('cats_dogs_best_model.h5')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SbHTqrJXKO5N",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "loss, score = model_.evaluate(x_test, y_test, verbose=0)\n",
        "print('now val loss¡ is %f, score is %f'%(loss, score))"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}